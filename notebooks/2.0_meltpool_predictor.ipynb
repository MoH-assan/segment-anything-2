{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Video segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {},
   "source": [
    "This notebook shows how to use SAM 2 for interactive segmentation in videos. It will cover the following:\n",
    "\n",
    "- adding clicks on a frame to get and refine _masklets_ (spatio-temporal masks) \n",
    "- propagating clicks to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video. \n",
    "\n",
    "If running locally using jupyter, first install `segment-anything-2` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything-2#installation) in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from ops.utils import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "sam2_checkpoint = \"../checkpoints/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "#### Select an example video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724",
   "metadata": {},
   "source": [
    "We assume that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
    "\n",
    "For your custom videos, you can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "where `-q:v` generates high-quality JPEG frames and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {},
   "source": [
    "#### Initialize the inference state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3216c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil  # Import shutil for file operations\n",
    "# Original directory where the video frames are stored\n",
    "\n",
    "masks_directory = \"/mnt/md126/users/mohamed/projects/AM/Data/RAW/ByDay/20240712/CRED/20240708_09_00/CREDgt_set_4/Masks\"\n",
    "raw_video_dir = \"/mnt/md126/users/mohamed/projects/AM/Data/RAW/ByDay/20240712/CRED/20240708_09_00/20240708_09_00_12072024_113101.raw\"\n",
    "\n",
    "_unique_id='gt4_v1_23p_32'\n",
    "################\n",
    "\n",
    "masks_directory = \"/mnt/md126/users/mohamed/projects/AM/Data/RAW/ByDay/20240712/CRED/20240708_09_00/CREDgt_set_4_v3/CREDgt_set_4/Masks/\"\n",
    "raw_video_dir = \"/mnt/md126/users/mohamed/projects/AM/Data/RAW/ByDay/20240712/CRED/20240708_09_00/20240708_09_00_12072024_113101.raw\"\n",
    "\n",
    "_unique_id='gt4_v3_ew8r3'\n",
    "################\n",
    "\n",
    "num_frames = 50\n",
    "skiprate = 50\n",
    "slected_mask_index = 5 # the index is the order of the mask in the list of masks xinyue has labeled. \n",
    "data_set_tag = f'CREDgt_set_4_20240708_09_00_12072024_113101_v1_selected_mask{slected_mask_index}_unique_id{_unique_id}'\n",
    "flood_masks = sort_and_filter_images(masks_directory, \"_c1\")   # Sort and filter the images\n",
    "voids_masks = sort_and_filter_images(masks_directory, \"_c2\")   # Sort and filter the images\n",
    "masks_frames = [int(name.split('_f_')[1].split('_')[0]) for name in flood_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_frame = masks_frames[slected_mask_index] # index based camera frame number\n",
    "flood_mask_path = flood_masks[slected_mask_index]\n",
    "voids_mask_path = voids_masks[slected_mask_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary directory to save reordered frames\n",
    "temp_dir = f\"./videos/{data_set_tag}/\"\n",
    "orginal_dir = f\"{temp_dir}/orginal/\"\n",
    "sam_temp_dir = f\"{temp_dir}/sam/orginal/\"\n",
    "sam_results_dir = f\"{temp_dir}/sam/results/\"\n",
    "xinyue_dir = f\"{temp_dir}/xinyue/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35da654",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"mask_frame_used\": slected_mask_index,\n",
    "    \"model_name\": \"SAM 2\",\n",
    "    \"raw_file_directory\": raw_video_dir,\n",
    "    \"masks_directory\": masks_directory,\n",
    "    \"num_frames\": num_frames,\n",
    "    \"skiprate\": skiprate,\n",
    "    \"unique_id\": _unique_id,\n",
    "    \"temp_dir\": temp_dir,\n",
    "    \"orginal_dir\": orginal_dir,\n",
    "    \"sam_temp_dir\": sam_temp_dir,\n",
    "    \"sam_results_dir\": sam_results_dir,\n",
    "    \"xinyue_dir\": xinyue_dir,\n",
    "    'sam2_checkpoint': sam2_checkpoint,\n",
    "    'model_cfg': model_cfg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66465eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)  # Remove the directory if it already exists\n",
    "os.makedirs(temp_dir, exist_ok=True) \n",
    "os.makedirs(xinyue_dir, exist_ok=True) \n",
    "os.makedirs(orginal_dir, exist_ok=True) \n",
    "os.makedirs(sam_temp_dir, exist_ok=True)\n",
    "os.makedirs(f\"{sam_results_dir}/masks/\", exist_ok=True) \n",
    "os.makedirs(f\"{sam_results_dir}/overlay/\", exist_ok=True) \n",
    "\n",
    "frames_indices = [ mask_frame ]\n",
    "\n",
    "for i in range(num_frames):\n",
    "    frames_indices.append(mask_frame + i*skiprate) # index based camera frame number\n",
    "frames_indices = list(set(frames_indices)) \n",
    "frames_indices.sort()\n",
    "mask_index = frames_indices.index(mask_frame) # sam index\n",
    "image_meta_data = {'filename_abs': raw_video_dir, 'height': 512, 'width': 640}\n",
    "mask_frame = open_frame_firstlight(image_meta_data, mask_frame)['img']\n",
    "pixel_wise_diff_list = []\n",
    "for i in frames_indices: # index based camera frame number\n",
    "    frame = open_frame_firstlight(image_meta_data, i)\n",
    "    img_array = frame['img']\n",
    "    pixel_wise_diff = np.linalg.norm(img_array - mask_frame)\n",
    "    pixel_wise_diff_list.append(pixel_wise_diff)\n",
    "    img = Image.fromarray(img_array)\n",
    "    img = img.convert('L')\n",
    "    img.save(f\"{orginal_dir}/{i}.jpg\") \n",
    "    img.save(f\"{sam_temp_dir}/{frames_indices.index(i)}.jpg\") # sam index 1,2,3,4,5,6,7,8,9,10,..........\n",
    "parameters['frames_indices'] = frames_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {},
   "source": [
    "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=sam_temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1f3f6-d74d-4016-934c-8d2a14d1a543",
   "metadata": {},
   "source": [
    "### Example 1: Segment & track one object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`.\n",
    "\n",
    "(The cell below is just for illustration; it's not needed to call `reset_state` here as this `inference_state` is just freshly initialized above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    "#### Step 1: Promting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and masks\n",
    "image = Image.fromarray(mask_frame) \n",
    "flood_mask = Image.open(os.path.join(masks_directory, flood_mask_path))\n",
    "voids_mask = Image.open(os.path.join(masks_directory, voids_mask_path))\n",
    "\n",
    "# Convert masks to binary arrays\n",
    "flood_mask_binary = np.array(flood_mask)[:, :, 0] > 250  # Extract red channel\n",
    "void_mask_binary = np.array(voids_mask)[:, :, 1] > 250  # Extract green channel\n",
    "\n",
    "# Create the meltpool and background masks\n",
    "meltpool_mask = flood_mask_binary & ~void_mask_binary\n",
    "background_mask = ~meltpool_mask\n",
    "\n",
    "# saving the masks\n",
    "void_mask_binary_img = Image.fromarray(void_mask_binary.astype(np.uint8) * 255)\n",
    "flood_mask_binary_img = Image.fromarray(flood_mask_binary.astype(np.uint8) * 255)\n",
    "meltpool_mask_img = Image.fromarray(meltpool_mask.astype(np.uint8) * 255)\n",
    "background_mask_img = Image.fromarray(background_mask.astype(np.uint8) * 255)\n",
    "\n",
    "flood_mask_binary_img.save(f\"{xinyue_dir}/flood_mask_binary.jpg\")\n",
    "void_mask_binary_img.save(f\"{xinyue_dir}/void_mask_binary.jpg\")\n",
    "meltpool_mask_img.save(f\"{xinyue_dir}/meltpool_mask.jpg\")\n",
    "background_mask_img.save(f\"{xinyue_dir}/background_mask.jpg\")\n",
    "img.save(f\"{xinyue_dir}/original.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "# sample_points_p = sample_points(meltpool_mask, 10)\n",
    "# sample_points_n = sample_points(background_mask, 10)\n",
    "# sample_points_n_2 = sample_points(void_mask_binary, 10) \n",
    "# points = np.concatenate([sample_points_p, sample_points_n, sample_points_n_2], axis=0)\n",
    "# labels = np.concatenate([np.ones(len(sample_points_p)), np.zeros(len(sample_points_n)), np.zeros(len(sample_points_n_2))], axis=0)\n",
    "\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "# _, out_obj_ids, out_mask_logits = predictor.add_new_points(\n",
    "#     inference_state=inference_state,\n",
    "#     frame_idx=frame_idx-1,\n",
    "#     obj_id=ann_obj_id,\n",
    "#     points=points,\n",
    "#     labels=labels,\n",
    "# )\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_mask(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=mask_index, # sam index\n",
    "    obj_id=ann_obj_id,\n",
    "    mask=meltpool_mask,\n",
    ")\n",
    "\n",
    "# Create a figure with 4x2 subplots\n",
    "fig, axs = plt.subplots(4, 2, figsize=(16, 24))\n",
    "# Display images and masks\n",
    "\n",
    "\n",
    "axs[0, 1].imshow(flood_mask, cmap=\"gray\")\n",
    "axs[0, 1].set_title(\"Xinyue Flood Mask\")\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "axs[1, 1].imshow(flood_mask_binary, cmap='gray')\n",
    "axs[1, 1].set_title('Binarized Xinyue Flood Mask')\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "axs[0, 0].imshow(voids_mask, cmap=\"gray\")\n",
    "axs[0, 0].set_title(\"Xinyue Voids Mask\")\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "\n",
    "axs[1, 0].imshow(void_mask_binary, cmap='gray')\n",
    "axs[1, 0].set_title('Binarized Xinyue Voids Mask')\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "axs[2, 1].imshow(meltpool_mask, cmap='gray')\n",
    "axs[2, 1].set_title('Prompt')\n",
    "axs[2, 1].axis('off')\n",
    "\n",
    "\n",
    "axs[2, 0].imshow(image, cmap=\"gray\")\n",
    "axs[2, 0].set_title(f\"Original Frame {mask_index}\")\n",
    "axs[2, 0].axis('off')\n",
    "\n",
    "\n",
    "axs[3, 0].imshow(image)\n",
    "from matplotlib.colors import ListedColormap\n",
    "single_color_cmap_blue = ListedColormap(['none', 'blue']) \n",
    "SAM2_Mask= (out_mask_logits[0] > 0.0).cpu().numpy().squeeze()\n",
    "axs[3, 0].imshow(SAM2_Mask, alpha=0.5,cmap=single_color_cmap_blue)  \n",
    "axs[3, 0].set_title('SAM2 Meltpool Mask Mask')\n",
    "axs[3, 0].axis('off')\n",
    "\n",
    "# Overlay meltpool mask on the original image\n",
    "single_color_cmap_red = ListedColormap(['none', 'red']) \n",
    "axs[3, 1].imshow(image, cmap='gray')\n",
    "axs[3, 1].imshow(meltpool_mask, alpha=0.5,cmap=single_color_cmap_red)  \n",
    "axs[3, 1].set_title('Xinque Meltpool Mask')\n",
    "axs[3, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{xinyue_dir}/all_masks.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52015ac-1b7b-4c59-bca3-c2b28484cf46",
   "metadata": {},
   "source": [
    "#### Step 3: Propagate the prompts to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b025bd-cd58-4bfb-9572-c8d2fd0a02ef",
   "metadata": {},
   "source": [
    "To get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e932-b0d5-4983-9718-6ee77d1ac31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming out_mask_logits are raw logits, and assuming we are only intrested in single object, we convert them to probabilities for confidence calculation\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "frame_metrics = {}  # mask_confidence contains the per-frame mask confidence scores\n",
    "frame_metrics['mask_confidence'] = {}\n",
    "frame_metrics['frame'] = {}\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "        # out_frame_idx is according sam index\n",
    "        \n",
    "        mask = (out_mask_logits[ann_obj_id-1] > 0.0).cpu().numpy() #\n",
    "        video_segments[out_frame_idx] = mask\n",
    "        # Convert logits to probabilities for the current mask\n",
    "        probabilities = sigmoid(out_mask_logits[ann_obj_id-1]).cpu().numpy()\n",
    "        frame_metrics['mask_confidence'][out_frame_idx] = probabilities[mask].mean()  # Mean confidence of the mask\n",
    "        frame_metrics['frame'] [out_frame_idx] = frames_indices[out_frame_idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(sam_temp_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af928482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming `image_meta_data`, `sam_temp_dir`, `frame_names`, `video_segments`, and `sam_results_dir` are defined\n",
    "vis_frame_stride = 1\n",
    "vis_dpi = 1200  # Define the DPI you're using for your images\n",
    "single_color_cmap_blue = ListedColormap(['none', 'blue'])\n",
    "width = image_meta_data['width']\n",
    "height = image_meta_data['height']\n",
    "parameters['vis_frame_stride'] = vis_frame_stride\n",
    "parameters['vis_dpi'] = vis_dpi\n",
    "parameters['width'] = width\n",
    "parameters['height'] = height\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate figure size in inches\n",
    "fig_width = width / vis_dpi\n",
    "fig_height = height / vis_dpi\n",
    "max_digits = len(str(999999999999))\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    # Create a figure with the computed dimensions\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=vis_dpi)\n",
    "    plt.imshow(Image.open(os.path.join(sam_temp_dir, frame_names[out_frame_idx])), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)  # Set padding to zero for tight layout\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)  # Remove any space between subplots\n",
    "    # Overlay the segmentation mask\n",
    "    out_mask = video_segments[out_frame_idx]\n",
    "    plt.imshow(out_mask.squeeze(), alpha=0.25, cmap=single_color_cmap_blue)\n",
    "    out_frame_idx_str = str(out_frame_idx).zfill(max_digits)\n",
    "    plt.savefig(f\"{sam_results_dir}/overlay/{out_frame_idx_str}_{frames_indices[out_frame_idx]}.png\")\n",
    "\n",
    "    # Save the mask alone\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=vis_dpi)\n",
    "    plt.imshow(out_mask.squeeze(), alpha=1, cmap=single_color_cmap_blue)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)  # Set padding to zero for tight layout\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)  # Remove any space between subplots\n",
    "    plt.savefig(f\"{sam_results_dir}/masks/{out_frame_idx_str}_{frames_indices[out_frame_idx]}.png\")\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the mask confidence scores from the dictionary\n",
    "mask_confidence_df = pd.DataFrame(frame_metrics)\n",
    "mask_confidence_df['pixel_wise_diff'] = pixel_wise_diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_dir = f\"{temp_dir}/mask_confidence.csv\"\n",
    "parameters['csv_file_dir'] = csv_file_dir\n",
    "mask_confidence_df.to_csv(csv_file_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON file\n",
    "json_file_path = os.path.join(temp_dir, \"parameters.json\")  # Saving in the same directory as your CSV\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(parameters, json_file, indent=4)\n",
    "print(f\"Parameters saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc492ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import widgets\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "def create_image_animation(folder_path, frame_rate=1):\n",
    "    \"\"\"Create an interactive image animation in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): Path to the folder containing images.\n",
    "    frame_rate (float): Number of seconds between frames.\n",
    "    \"\"\"\n",
    "    # List all files in the directory and sort them\n",
    "    files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    # print(f\"Found {len(files)} images in {folder_path}\")\n",
    "    # print(f'Files are {files}')\n",
    "    # Display widget setup\n",
    "    output = widgets.Output()\n",
    "    play_button = widgets.Button(description=\"Play\")\n",
    "    stop_button = widgets.Button(description=\"Stop\")\n",
    "    slider = widgets.IntSlider(value=0, min=0, max=len(files) - 1, step=1, description='Frame:')\n",
    "    play_control = widgets.Play(value=0, min=0, max=len(files) - 1, interval=1000 * frame_rate, show_repeat=False)\n",
    "    widgets.jslink((play_control, 'value'), (slider, 'value'))  # Link slider and play widget\n",
    "\n",
    "    # Event handlers for buttons\n",
    "    def play_animation(b):\n",
    "        play_control._playing = True\n",
    "\n",
    "    def stop_animation(b):\n",
    "        play_control._playing = False\n",
    "\n",
    "    play_button.on_click(play_animation)\n",
    "    stop_button.on_click(stop_animation)\n",
    "\n",
    "    # Function to display the current frame\n",
    "    def show_current_frame(change):\n",
    "        frame_index = change['new']\n",
    "        if output.outputs:  # if there is something already displayed\n",
    "            output.clear_output(wait=True)\n",
    "        with output:\n",
    "            display(Image.open(files[frame_index]))\n",
    "\n",
    "    slider.observe(show_current_frame, names='value')\n",
    "\n",
    "    # Arrange buttons and slider\n",
    "    control_panel = widgets.HBox([play_button, stop_button, slider])\n",
    "    display(control_panel, play_control, output)\n",
    "\n",
    "    # Initialize\n",
    "    show_current_frame({'new': slider.value})\n",
    "\n",
    "# Example usage\n",
    "# create_image_animation('/path/to/your/image/directory', frame_rate=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_image_animation(f\"{sam_results_dir}/overlay/\", frame_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9026560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
