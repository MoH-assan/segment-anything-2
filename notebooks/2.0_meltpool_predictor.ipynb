{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Video segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {},
   "source": [
    "This notebook shows how to use SAM 2 for interactive segmentation in videos. It will cover the following:\n",
    "\n",
    "- adding clicks on a frame to get and refine _masklets_ (spatio-temporal masks) \n",
    "- propagating clicks to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video. \n",
    "\n",
    "If running locally using jupyter, first install `segment-anything-2` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything-2#installation) in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "sam2_checkpoint = \"../checkpoints/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def sort_and_filter_images(directory, keyword):\n",
    "    \"\"\"\n",
    "    Scans a directory for JPEG images, filters those that include a specific keyword in the filename,\n",
    "    and sorts them based on the integer value that appears after 'f_' in the filename.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The path to the directory containing the images.\n",
    "    - keyword (str): The keyword to filter filenames by (e.g., '_c1').\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of filtered and sorted filenames.\n",
    "    \"\"\"\n",
    "    # Scan all the JPEG frame names in the directory\n",
    "    frame_names = [\n",
    "        p for p in os.listdir(directory)\n",
    "        if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]\n",
    "    ]\n",
    "\n",
    "    # Filter to include only images that contain the keyword\n",
    "    filtered_frames = [name for name in frame_names if keyword in name]\n",
    "\n",
    "    # Sort by the integer number after 'f_'\n",
    "    filtered_frames.sort(key=lambda name: int(name.split('_f_')[1].split('_')[0]))\n",
    "\n",
    "    return filtered_frames\n",
    "def sample_points(mask, num_samples):\n",
    "    if num_samples == 0:\n",
    "        return np.array([[]])\n",
    "    \"\"\"Sample points where the mask is True.\"\"\"\n",
    "    # Find indices where the mask is True\n",
    "    y_indices, x_indices = np.where(mask)\n",
    "    total_points = len(x_indices)\n",
    "    if num_samples > total_points:\n",
    "        raise ValueError(f\"num_samples ({num_samples}) should be less than or equal to the total number of points ({total_points}).\")\n",
    "    # Randomly select `num_samples` points\n",
    "    chosen_indices = np.random.choice(len(x_indices), num_samples, replace=False)\n",
    "    # Extract the corresponding points\n",
    "    sampled_points = np.array(list(zip(x_indices[chosen_indices], y_indices[chosen_indices])), dtype=np.float32)\n",
    "    return sampled_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_frame_firstlight(meta_data, index):\n",
    "    header_size=0\n",
    "    # Calculate the size of each image in bytes\n",
    "    img_size = meta_data['height'] * meta_data['width']\n",
    "    # Calculate the offset to the desired image\n",
    "    \n",
    "    with open(meta_data['filename_abs'], 'rb') as f:\n",
    "        f.seek(0, 2)  # Move the cursor to the end of the file\n",
    "        total_size = f.tell()  # `tell` gives you the current position, which is the size\n",
    "        #print(\"Total file size using file object:\", total_size, \"bytes\")\n",
    "    offset = header_size + np.int64(img_size) * 2 * np.int64(index)  # 2 bytes per pixel for 16-bit images\n",
    "    with open(meta_data['filename_abs'], 'rb') as f:\n",
    "        # Seek to the desired position\n",
    "        f.seek(offset)\n",
    "        # Read the image data and reshape it\n",
    "        img_data = np.fromfile(f, dtype=np.int16, count=img_size)  # read as 16-bit signed integers\n",
    "        img = img_data.reshape(meta_data['height'], meta_data['width'])\n",
    "        #print(f'relative_frame  = {index}, first four {img[0,:4]}')\n",
    "        # Extract the first two pixels from the first row\n",
    "        first_pixel = img[0, 0]\n",
    "        second_pixel = img[0, 1]\n",
    "\n",
    "        # Combine the first and second pixels to form a 32-bit integer frame counter\n",
    "        # Convert the pixels to 32-bit integers to avoid issues with negative values and bit manipulation\n",
    "        frame_counter = (second_pixel.astype(np.int32) << 16) | (first_pixel & 0xFFFF)\n",
    "\n",
    "        #print(\"Frame counter:\", frame_counter)\n",
    "        img[0, :4] = 0    #Removing the tag \n",
    "        white_pixel = 7000\n",
    "        black_pixel = 0\n",
    "        img[img > white_pixel] = white_pixel\n",
    "        img[img < black_pixel] = black_pixel\n",
    "        img_normalized = 255 * (img.astype(np.int32)) / (white_pixel) #TODO check if this is the correct normalization to keep the details and avoid artifical oversaturation\n",
    "        image_plus_camera_frame_counter = {'camera_frame_counter': frame_counter, 'img': img_normalized}\n",
    "    return image_plus_camera_frame_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "#### Select an example video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724",
   "metadata": {},
   "source": [
    "We assume that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
    "\n",
    "For your custom videos, you can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "where `-q:v` generates high-quality JPEG frames and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {},
   "source": [
    "#### Initialize the inference state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3216c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil  # Import shutil for file operations\n",
    "# Original directory where the video frames are stored\n",
    "video_dir1 = \"/mnt/md126/users/mohamed/projects/AM/Data/RAW/ByDay/20240712/CRED/20240708_09_00/CREDgt_set_4/Original\"\n",
    "masks_directory = \"/mnt/md126/users/mohamed/projects/AM/Data/RAW/ByDay/20240712/CRED/20240708_09_00/CREDgt_set_4/Masks\"\n",
    "raw_video_dir = \"/mnt/md126/users/mohamed/projects/AM/Data/RAW/ByDay/20240712/CRED/20240708_09_00/20240708_09_00_12072024_113101.raw\"\n",
    "video_dir2 = \"/mnt/md126/users/mohamed/projects/AM/Data/Processed/JPEG/ByDay/20240712/gt_set_4\"\n",
    "num_frames = 50\n",
    "skiprate = 1\n",
    "slected_mask_index = 2 # the index is the order of the mask in the list of masks xinyue has labeled. \n",
    "data_set_tag = f'CREDgt_set_4_20240708_09_00_12072024_113101_v1_selected_mask{slected_mask_index}'\n",
    "flood_masks = sort_and_filter_images(masks_directory, \"_c1\")   # Sort and filter the images\n",
    "voids_masks = sort_and_filter_images(masks_directory, \"_c2\")   # Sort and filter the images\n",
    "masks_frames = [int(name.split('_f_')[1].split('_')[0]) for name in flood_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_frame = masks_frames[slected_mask_index] # index based camera frame number\n",
    "flood_mask_path = flood_masks[slected_mask_index]\n",
    "voids_mask_path = voids_masks[slected_mask_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66465eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary directory to save reordered frames\n",
    "temp_dir = f\"./videos/{data_set_tag}/\"\n",
    "orginal_dir = f\"{temp_dir}/orginal/\"\n",
    "sam_temp_dir = f\"{temp_dir}/sam/orginal/\"\n",
    "sam_results_dir = f\"{temp_dir}/sam/results/\"\n",
    "xinyue_dir = f\"{temp_dir}/xinyue/\"\n",
    "\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)  # Remove the directory if it already exists\n",
    "os.makedirs(temp_dir, exist_ok=True) \n",
    "os.makedirs(xinyue_dir, exist_ok=True) \n",
    "os.makedirs(orginal_dir, exist_ok=True) \n",
    "os.makedirs(sam_temp_dir, exist_ok=True)\n",
    "os.makedirs(f\"{sam_results_dir}/masks/\", exist_ok=True) \n",
    "os.makedirs(f\"{sam_results_dir}/overlay/\", exist_ok=True) \n",
    "\n",
    "frames_indices = [ mask_frame ]\n",
    "\n",
    "for i in range(num_frames):\n",
    "    frames_indices.append(mask_frame + i*skiprate) # index based camera frame number\n",
    "frames_indices = list(set(frames_indices)) \n",
    "frames_indices.sort()\n",
    "mask_index = frames_indices.index(mask_frame) # sam index\n",
    "image_meta_data = {'filename_abs': raw_video_dir, 'height': 512, 'width': 640}\n",
    "mask_frame = open_frame_firstlight(image_meta_data, mask_frame)['img']\n",
    "pixel_wise_diff_list = []\n",
    "for i in frames_indices: # index based camera frame number\n",
    "    frame = open_frame_firstlight(image_meta_data, i)\n",
    "    img_array = frame['img']\n",
    "    pixel_wise_diff = np.linalg.norm(img_array - mask_frame)\n",
    "    pixel_wise_diff_list.append(pixel_wise_diff)\n",
    "    img = Image.fromarray(img_array)\n",
    "    img = img.convert('L')\n",
    "    img.save(f\"{orginal_dir}/{i}.jpg\") \n",
    "    img.save(f\"{sam_temp_dir}/{frames_indices.index(i)}.jpg\") # sam index 1,2,3,4,5,6,7,8,9,10,..........\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {},
   "source": [
    "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=sam_temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1f3f6-d74d-4016-934c-8d2a14d1a543",
   "metadata": {},
   "source": [
    "### Example 1: Segment & track one object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`.\n",
    "\n",
    "(The cell below is just for illustration; it's not needed to call `reset_state` here as this `inference_state` is just freshly initialized above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    "#### Step 1: Promting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and masks\n",
    "image = Image.fromarray(mask_frame) \n",
    "flood_mask = Image.open(os.path.join(masks_directory, flood_mask_path))\n",
    "voids_mask = Image.open(os.path.join(masks_directory, voids_mask_path))\n",
    "\n",
    "# Convert masks to binary arrays\n",
    "flood_mask_binary = np.array(flood_mask)[:, :, 0] > 200  # Extract red channel\n",
    "void_mask_binary = np.array(voids_mask)[:, :, 1] > 200  # Extract green channel\n",
    "\n",
    "# Create the meltpool and background masks\n",
    "meltpool_mask = flood_mask_binary & ~void_mask_binary\n",
    "background_mask = ~meltpool_mask\n",
    "\n",
    "# saving the masks\n",
    "void_mask_binary_img = Image.fromarray(void_mask_binary.astype(np.uint8) * 255)\n",
    "flood_mask_binary_img = Image.fromarray(flood_mask_binary.astype(np.uint8) * 255)\n",
    "meltpool_mask_img = Image.fromarray(meltpool_mask.astype(np.uint8) * 255)\n",
    "background_mask_img = Image.fromarray(background_mask.astype(np.uint8) * 255)\n",
    "\n",
    "flood_mask_binary_img.save(f\"{xinyue_dir}/flood_mask_binary.jpg\")\n",
    "void_mask_binary_img.save(f\"{xinyue_dir}/void_mask_binary.jpg\")\n",
    "meltpool_mask_img.save(f\"{xinyue_dir}/meltpool_mask.jpg\")\n",
    "background_mask_img.save(f\"{xinyue_dir}/background_mask.jpg\")\n",
    "img.save(f\"{xinyue_dir}/original.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "# sample_points_p = sample_points(meltpool_mask, 10)\n",
    "# sample_points_n = sample_points(background_mask, 10)\n",
    "# sample_points_n_2 = sample_points(void_mask_binary, 10) \n",
    "# points = np.concatenate([sample_points_p, sample_points_n, sample_points_n_2], axis=0)\n",
    "# labels = np.concatenate([np.ones(len(sample_points_p)), np.zeros(len(sample_points_n)), np.zeros(len(sample_points_n_2))], axis=0)\n",
    "\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "# _, out_obj_ids, out_mask_logits = predictor.add_new_points(\n",
    "#     inference_state=inference_state,\n",
    "#     frame_idx=frame_idx-1,\n",
    "#     obj_id=ann_obj_id,\n",
    "#     points=points,\n",
    "#     labels=labels,\n",
    "# )\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_mask(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=mask_index, # sam index\n",
    "    obj_id=ann_obj_id,\n",
    "    mask=meltpool_mask,\n",
    ")\n",
    "\n",
    "# Create a figure with 3x3 subplots to include additional visualizations\n",
    "fig, axs = plt.subplots(3, 3, figsize=(18, 18))\n",
    "fig.suptitle('Detailed Analysis with Interactions')\n",
    "\n",
    "# First row: Original image and two masks\n",
    "axs[0, 0].imshow(image, cmap=\"gray\")\n",
    "axs[0, 0].set_title(f\"Original Frame {mask_index}\")\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "axs[0, 1].imshow(flood_mask, cmap=\"gray\")\n",
    "axs[0, 1].set_title(\"Flood Mask\")\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "axs[0, 2].imshow(voids_mask, cmap=\"gray\")\n",
    "axs[0, 2].set_title(\"Voids Mask\")\n",
    "axs[0, 2].axis('off')\n",
    "\n",
    "# Second row: Binarized masks\n",
    "axs[1, 0].imshow(flood_mask_binary, cmap='gray')\n",
    "axs[1, 0].set_title('Binarized Flood Mask')\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "axs[1, 1].imshow(void_mask_binary, cmap='gray')\n",
    "axs[1, 1].set_title('Binarized Voids Mask')\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "axs[1, 2].imshow(meltpool_mask, cmap='gray')\n",
    "axs[1, 2].set_title('Meltpool Mask')\n",
    "axs[1, 2].axis('off')\n",
    "\n",
    "\n",
    "# axs[2, 0].imshow(image)\n",
    "# show_points(points, labels, axs[2, 0])  # Assuming points and labels are defined\n",
    "axs[2, 0].axis('off')\n",
    "\n",
    "axs[2, 1].imshow(image)\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), axs[2, 1], out_obj_ids[0])  # Assuming mask logits and object IDs are defined\n",
    "\n",
    "# Hide the last subplot as it's unused\n",
    "axs[2, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52015ac-1b7b-4c59-bca3-c2b28484cf46",
   "metadata": {},
   "source": [
    "#### Step 3: Propagate the prompts to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b025bd-cd58-4bfb-9572-c8d2fd0a02ef",
   "metadata": {},
   "source": [
    "To get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e932-b0d5-4983-9718-6ee77d1ac31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming out_mask_logits are raw logits, and assuming we are only intrested in single object, we convert them to probabilities for confidence calculation\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "frame_metrics = {}  # mask_confidence contains the per-frame mask confidence scores\n",
    "frame_metrics['mask_confidence'] = {}\n",
    "frame_metrics['frame'] = {}\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "        # out_frame_idx is according sam index\n",
    "        \n",
    "        mask = (out_mask_logits[ann_obj_id-1] > 0.0).cpu().numpy() #\n",
    "        video_segments[out_frame_idx] = mask\n",
    "        # Convert logits to probabilities for the current mask\n",
    "        probabilities = sigmoid(out_mask_logits[ann_obj_id-1]).cpu().numpy()\n",
    "        frame_metrics['mask_confidence'][out_frame_idx] = probabilities[mask].mean()  # Mean confidence of the mask\n",
    "        frame_metrics['frame'] [out_frame_idx] = frames_indices[out_frame_idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(sam_temp_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "frame_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af928482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 1\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(sam_temp_dir, frame_names[out_frame_idx])))\n",
    "    out_mask = video_segments[out_frame_idx]    \n",
    "    show_mask(out_mask, plt.gca(), obj_id=ann_obj_id)\n",
    "    \n",
    "    plt.savefig(f\"{sam_results_dir}/overlay/{out_frame_idx}.jpg\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(out_mask.squeeze())\n",
    "    plt.savefig(f\"{sam_results_dir}/masks/{out_frame_idx}.jpg\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the mask confidence scores from the dictionary\n",
    "mask_confidence_df = pd.DataFrame(frame_metrics)\n",
    "mask_confidence_df['pixel_wise_diff'] = pixel_wise_diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_confidence_df.to_csv(f\"{temp_dir}/mask_confidence.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
